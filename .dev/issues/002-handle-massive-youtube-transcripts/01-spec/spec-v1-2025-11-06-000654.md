## üìã SPEC: Handle Massive YouTube Transcripts (5-6 Hours)

**Approach:** BROWNFIELD
**Tech Stack:** TypeScript, Node.js, youtube-transcript library, fs/promises, he (HTML entity decoder)
**Feature Type:** BACKEND

---

### What We're Building

Memory-efficient transcript processing system for YouTube MCP server that handles very long videos (5-6 hours) using streaming file writes and chunked processing, preventing memory exhaustion and timeout failures.

---

### User Flow

1. User requests transcript via MCP tool for 5-6 hour video
2. Server fetches transcript from YouTube API (returns full array of entries)
3. System processes transcript in configurable chunks (e.g., 1000 entries per batch)
4. Each chunk is decoded and streamed directly to output file
5. Progress feedback provided for transcripts exceeding 5000 entries
6. File write completes successfully, transcript ready for consumption

---

### Requirements

**Must Have:**
- Stream transcript to file using `fs.createWriteStream()` instead of loading entire content in memory
- Process transcript entries in configurable chunks (default: 1000 entries per batch)
- Support transcripts of any length without memory errors (tested up to 360k words)
- Provide progress feedback for transcripts with >5000 entries
- Maintain stable memory usage regardless of transcript length
- Complete file writes successfully for files >2MB

**Should Have:**
- Configurable chunk size for different use cases
- Graceful timeout handling with clear error messages for YouTube API failures
- Progress percentage calculation based on total entries
- Memory usage monitoring/logging for debugging

**Must NOT:**
- ‚ùå Load entire transcript into single string via `.join()` operation
- ‚ùå Build full markdown content in memory before writing
- ‚ùå Use `fs.writeFile()` for large content (atomic write causes memory spike)
- ‚ùå Degrade performance for normal-length videos (<1 hour)

---

### Acceptance Criteria

**AC1: Large Transcript Processing**
- Given: User requests transcript for 5-hour video (~300k words, ~50k entries)
- When: Server processes the transcript
- Then: Memory usage stays below 100MB peak, file writes successfully, no timeout errors

**AC2: Extra-Large Transcript Processing**
- Given: User requests transcript for 6-hour video (~360k words, ~60k entries)
- When: Server processes the transcript
- Then: Memory usage stays below 100MB peak, file writes successfully, no timeout errors

**AC3: Progress Feedback for Long Operations**
- Given: Transcript has >5000 entries
- When: Processing begins
- Then: Progress updates logged every N chunks (e.g., "Progress: 5000/50000 entries")

**AC4: Streaming File Write**
- Given: Any transcript request
- When: Server begins writing output
- Then: Uses `fs.createWriteStream()` with chunked writes, not atomic `fs.writeFile()`

**AC5: No Performance Regression**
- Given: User requests transcript for normal 30-minute video (~1800 words)
- When: Server processes transcript
- Then: Completes in similar time as current implementation (no slowdown)

**AC6: Error Handling**
- Given: YouTube API times out or returns error
- When: Transcript fetch fails
- Then: Clear error message distinguishes timeout from other failures

---

### Implementation Notes

**[For BROWNFIELD Projects]:**

**Files to modify:**
- `servers/binaries/youtube-mcp-server/src/index.ts:114-173` (main transcript processing logic)

**Current implementation pattern:**
```typescript
// Line 114-116: Fetch returns full array
const transcriptEntries = await YoutubeTranscript.fetchTranscript(video_url);

// Line 133: All entries joined into single string
const rawTranscriptText = transcriptEntries.map((entry) => entry.text).join(' ');

// Line 138: Entire text decoded at once
const decodedTranscriptText = he.decode(preDecodedText);

// Line 146: Full markdown created in memory
const markdownContent = `# ${title}\n\n${decodedTranscriptText}`;

// Line 173: Atomic write
await fs.writeFile(absoluteOutputPath, markdownContent, 'utf-8');
```

**Replacement pattern (streaming approach):**
```typescript
import { createWriteStream } from 'fs';

// Configuration
const CHUNK_SIZE = 1000; // entries per batch
const PROGRESS_THRESHOLD = 5000; // when to show progress

// Fetch transcript (unavoidable - library returns full array)
const transcriptEntries = await YoutubeTranscript.fetchTranscript(video_url);

// Create write stream
const writeStream = createWriteStream(absoluteOutputPath, { encoding: 'utf-8' });

// Write header
writeStream.write(`# ${title}\n\n`);

// Process and write in chunks
for (let i = 0; i < transcriptEntries.length; i += CHUNK_SIZE) {
  const chunk = transcriptEntries.slice(i, i + CHUNK_SIZE);
  
  // Decode chunk text
  const chunkText = chunk
    .map(entry => he.decode(entry.text))
    .join(' ');
  
  // Write chunk to stream
  writeStream.write(chunkText + ' ');
  
  // Progress feedback for long transcripts
  if (transcriptEntries.length > PROGRESS_THRESHOLD) {
    console.error(`Progress: ${Math.min(i + CHUNK_SIZE, transcriptEntries.length)}/${transcriptEntries.length} entries`);
  }
}

// Close stream
await new Promise<void>((resolve, reject) => {
  writeStream.end(() => resolve());
  writeStream.on('error', reject);
});
```

**Integration points:**
- Replace lines 133-173 in `servers/binaries/youtube-mcp-server/src/index.ts`
- Preserve existing error handling for video metadata fetch (lines 114-130)
- Maintain file path resolution logic (lines 160-171)
- Keep return value structure unchanged (lines 175-181)

**Testing approach:**
- Create test suite in `.dev/testing/test_youtube_transcript_streaming.ts`
- Test with real YouTube videos at 1hr, 3hr, 5hr, 6hr durations
- Monitor memory usage using `process.memoryUsage()` before/after
- Verify output file integrity (file size matches expected word count)
- Validate no performance regression for short videos (<1hr)

**Configuration:**
- No new environment variables required
- Chunk size hardcoded initially (can make configurable later)
- Progress threshold hardcoded at 5000 entries

**Dependencies:**
- Existing `youtube-transcript: ^1.2.1` (no changes needed)
- Existing `he: ^1.2.0` (no changes needed)
- Add `import { createWriteStream }` from `fs` (Node.js built-in)

**Migration strategy:**
- No database changes required
- No API changes required (output format unchanged)
- Backward compatible with existing MCP clients

---

### Open Questions

- [ ] Should chunk size be configurable via environment variable or tool parameter?
- [ ] Should progress updates be sent to MCP client or just logged to stderr?
- [ ] Should we add timeout configuration for YouTube API fetch (currently no timeout)?

### Out of Scope

- Forking/enhancing youtube-transcript library for true streaming (library limitation)
- Batch processing multiple transcripts (issue #1 dependency)
- YouTube API rate limiting/throttling (separate concern)
- Transcript caching/resumption after failures
