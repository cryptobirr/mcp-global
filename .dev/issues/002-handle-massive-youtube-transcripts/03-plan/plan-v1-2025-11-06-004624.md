# Implementation Plan: Handle Massive YouTube Transcripts (5-6 Hours)

**Issue:** #2
**Approach:** BROWNFIELD
**Created:** 2025-11-06T00:46:24Z
**Version:** v1

---

## Summary

Replace memory-intensive atomic file write with streaming file writes for YouTube transcript processing. Current implementation loads entire transcript into memory (causing 13-17MB peak for 6-hour videos), then writes atomically. New implementation streams transcript to disk in 1000-entry chunks with progress feedback, maintaining <100MB peak memory usage regardless of transcript length.

---

## Architectural Decisions

### AD-1: Environment Variable Gated Memory Logging

**Decision:** Use `DEBUG=memory` environment variable to gate memory usage logging

**Options Considered:**
- Option A: Built-in `process.memoryUsage()` in production code (always on)
- Option B: Test-only memory profiling (no production visibility)
- Option C: Environment variable gated logging (selected)

**Rationale:**
- Provides production debugging capability when needed via `DEBUG=memory` flag
- Clean default output (no memory stats spam for typical users)
- Follows Node.js ecosystem conventions (`DEBUG=` pattern is standard)
- Test validation possible by setting `process.env.DEBUG='memory'` in test suite
- Negligible performance overhead (`if (process.env.DEBUG)` check is trivial)

**Trade-offs:**
- Slightly more complex than Option A (env var check required)
- Requires documentation for users to discover `DEBUG=memory` flag
- Acceptable: Production visibility justifies minimal complexity increase

**Implementation:**
```typescript
// Before streaming
if (process.env.DEBUG?.includes('memory')) {
  const before = process.memoryUsage();
  console.error(`Memory before streaming: ${Math.round(before.heapUsed / 1024 / 1024)}MB`);
}

// ... streaming logic ...

// After streaming
if (process.env.DEBUG?.includes('memory')) {
  const after = process.memoryUsage();
  console.error(`Memory after streaming: ${Math.round(after.heapUsed / 1024 / 1024)}MB`);
  console.error(`Peak memory delta: ${Math.round((after.heapUsed - before.heapUsed) / 1024 / 1024)}MB`);
}
```

---

### AD-2: Fixed Entry Interval Progress Logging (Every 5000 Entries)

**Decision:** Log progress every 5000 entries processed

**Options Considered:**
- Option A: Fixed chunk interval (every N chunks)
- Option B: Percentage-based logging (every N%)
- Option C: Fixed entry interval (every 5000 entries) (selected)

**Rationale:**
- Simplest implementation: single modulo check, no state tracking
- Aligns with spec threshold (AC3 specifies progress for >5000 entries)
- Acceptable log frequency: 60k entries → 12 logs (not spam)
- Minimal performance overhead (modulo operation per chunk)
- Consistent interval regardless of transcript length

**Trade-offs:**
- Fixed log count doesn't scale with transcript length (unlike Option B)
- May log less frequently for shorter transcripts (10k entries = 2 logs)
- Acceptable: Simplicity and spec alignment justify fixed interval

**Implementation:**
```typescript
for (let i = 0; i < transcriptEntries.length; i += CHUNK_SIZE) {
  // ... chunk processing ...

  // Progress logging every 5000 entries
  if (transcriptEntries.length > PROGRESS_THRESHOLD && (i + CHUNK_SIZE) % 5000 === 0) {
    const processed = Math.min(i + CHUNK_SIZE, transcriptEntries.length);
    console.error(`Progress: ${processed}/${transcriptEntries.length} entries`);
  }
}
```

---

## Files to Modify

### File: `servers/binaries/youtube-mcp-server/src/index.ts`

**Current responsibility:** MCP server for YouTube transcript fetching, formatting, and file saving

**Lines to modify:** 1-13 (imports), 133-173 (transcript processing and file write)

**Change needed:**
1. Add `createWriteStream` import from `fs`
2. Replace atomic write (`fs.writeFile()`) with streaming write (`createWriteStream()`)
3. Process transcript in 1000-entry chunks instead of loading entire text into memory
4. Add progress logging for transcripts >5000 entries
5. Add environment-gated memory logging
6. Add stream error handling with partial file cleanup

**Existing patterns to follow:**
- **Error logging pattern:** Use `console.error()` for all diagnostic output (stdout reserved for MCP protocol) - see lines 46, 106, 113, 131, 169, 172, 186, 220
- **Error handling pattern:** Try-catch with `McpError` types (`ErrorCode.InternalError`) - see lines 85-96, 185-211
- **Path resolution:** Use `path.resolve(CLINE_CWD, ...)` for absolute paths, `path.relative(CLINE_CWD, ...)` for return values - see lines 165-166, 176
- **Directory creation:** Use `fs.mkdir(outputDir, { recursive: true })` before file operations - see line 170
- **String sanitization:** Use chained `.replace()` calls for cleaning special characters - see lines 135-136, 150-161
- **Type guards:** Validate arguments with type predicate functions - see lines 18-24, 91-96

**Current Code (lines 1-13):**
```typescript
#!/usr/bin/env node
import { Server } from '@modelcontextprotocol/sdk/server/index.js';
import { StdioServerTransport } from '@modelcontextprotocol/sdk/server/stdio.js';
import {
  CallToolRequestSchema,
  ErrorCode,
  ListToolsRequestSchema,
  McpError,
} from '@modelcontextprotocol/sdk/types.js';
import { YoutubeTranscript } from 'youtube-transcript';
import fs from 'fs/promises';
import path from 'path';
import he from 'he'; // Import the 'he' library
```

**Target Code (lines 1-14):**
```typescript
#!/usr/bin/env node
import { Server } from '@modelcontextprotocol/sdk/server/index.js';
import { StdioServerTransport } from '@modelcontextprotocol/sdk/server/stdio.js';
import {
  CallToolRequestSchema,
  ErrorCode,
  ListToolsRequestSchema,
  McpError,
} from '@modelcontextprotocol/sdk/types.js';
import { YoutubeTranscript } from 'youtube-transcript';
import fs from 'fs/promises';
import { createWriteStream } from 'fs';
import path from 'path';
import he from 'he'; // Import the 'he' library
```

**Pattern Adherence:** Preserves existing import grouping (MCP SDK → external libs → Node.js built-ins)

---

**Current Code (lines 130-173):**
```typescript
// 2. Format transcript to Markdown and generate title
console.error('Formatting transcript and generating title...');
// Manually replace common apostrophe entities first, then decode others
const rawTranscriptText = transcriptEntries.map((entry) => entry.text).join(' ');
const preDecodedText = rawTranscriptText
    .replace(/&#39;/g, "'") // Replace numeric entity
    .replace(/'/g, "'"); // Replace named entity
// Use preDecodedText for filename generation, decode fully for content
const decodedTranscriptText = he.decode(preDecodedText);


// Generate title from the first ~10 words of the *fully* decoded text
const titleWords = decodedTranscriptText.split(' ').slice(0, 10).join(' ');
const title = titleWords ? titleWords.trim() + '...' : 'Transcript'; // Add ellipsis if truncated

// Use *fully* decoded text for the file content
const markdownContent = `# ${title}\n\n${decodedTranscriptText}`;

// Generate a sanitized filename from the first ~5 words of the *pre-decoded* text
const filenameWords = preDecodedText.split(' ').slice(0, 5).join(' '); // Use preDecodedText here
let baseFilename = filenameWords
    ? filenameWords
        .trim()
        .toLowerCase()
        // First, replace spaces with hyphens
        .replace(/\s+/g, '-')
        // Then, remove any character that is NOT a letter, number, or hyphen
        .replace(/[^a-z0-9-]/g, '')
    : `transcript-${Date.now()}`; // Fallback filename
if (!baseFilename || baseFilename === '-') { // Handle cases where sanitization results in empty string or just hyphens
    baseFilename = `transcript-${Date.now()}`;
}
const finalFilename = `${baseFilename}.md`;

// Construct the final path using the original output_path's directory
const originalOutputDir = path.dirname(path.resolve(CLINE_CWD, output_path));
const absoluteOutputPath = path.join(originalOutputDir, finalFilename);
const outputDir = path.dirname(absoluteOutputPath); // Should be the same as originalOutputDir

console.error(`Ensuring directory exists: ${outputDir}`);
await fs.mkdir(outputDir, { recursive: true });

console.error(`Saving transcript to: ${absoluteOutputPath}`);
await fs.writeFile(absoluteOutputPath, markdownContent, 'utf-8');
```

**Target Code (lines 130-225):**
```typescript
// 2. Format transcript to Markdown and generate title/filename
console.error('Formatting transcript and generating title...');

// Configuration constants
const CHUNK_SIZE = 1000; // entries per batch
const PROGRESS_THRESHOLD = 5000; // when to show progress

// Memory monitoring (gated by DEBUG env var)
let memoryBefore: NodeJS.MemoryUsage | undefined;
if (process.env.DEBUG?.includes('memory')) {
  memoryBefore = process.memoryUsage();
  console.error(`Memory before streaming: ${Math.round(memoryBefore.heapUsed / 1024 / 1024)}MB`);
}

// Generate title from first entry only (avoid loading full transcript)
const firstEntryText = transcriptEntries[0]?.text || '';
const preDecodedFirstEntry = firstEntryText
    .replace(/&#39;/g, "'") // Replace numeric entity
    .replace(/'/g, "'"); // Replace named entity
const decodedFirstEntry = he.decode(preDecodedFirstEntry);

// Generate title from first ~10 words
const titleWords = decodedFirstEntry.split(' ').slice(0, 10).join(' ');
const title = titleWords ? titleWords.trim() + '...' : 'Transcript';

// Generate sanitized filename from first ~5 words of first entry
const filenameWords = preDecodedFirstEntry.split(' ').slice(0, 5).join(' ');
let baseFilename = filenameWords
    ? filenameWords
        .trim()
        .toLowerCase()
        .replace(/\s+/g, '-')
        .replace(/[^a-z0-9-]/g, '')
    : `transcript-${Date.now()}`;
if (!baseFilename || baseFilename === '-') {
    baseFilename = `transcript-${Date.now()}`;
}
const finalFilename = `${baseFilename}.md`;

// Construct final path
const originalOutputDir = path.dirname(path.resolve(CLINE_CWD, output_path));
const absoluteOutputPath = path.join(originalOutputDir, finalFilename);
const outputDir = path.dirname(absoluteOutputPath);

console.error(`Ensuring directory exists: ${outputDir}`);
await fs.mkdir(outputDir, { recursive: true });

console.error(`Saving transcript to: ${absoluteOutputPath}`);

// Create write stream
const writeStream = createWriteStream(absoluteOutputPath, { encoding: 'utf-8' });

// Error handling: cleanup partial file on stream errors
writeStream.on('error', async (err: Error) => {
  console.error('Stream write error:', err);

  // Cleanup partial file
  try {
    await fs.unlink(absoluteOutputPath);
    console.error(`Cleaned up partial file: ${absoluteOutputPath}`);
  } catch (unlinkErr) {
    console.error('Failed to cleanup partial file:', unlinkErr);
  }

  throw new McpError(
    ErrorCode.InternalError,
    `Failed to write transcript: ${err.message}`
  );
});

// Write markdown header
writeStream.write(`# ${title}\n\n`);

// Process and write transcript in chunks
for (let i = 0; i < transcriptEntries.length; i += CHUNK_SIZE) {
  const chunk = transcriptEntries.slice(i, i + CHUNK_SIZE);

  // Decode chunk text (per entry to avoid large string concat)
  const chunkText = chunk
    .map(entry => {
      const preDecoded = entry.text
        .replace(/&#39;/g, "'")
        .replace(/'/g, "'");
      return he.decode(preDecoded);
    })
    .join(' ');

  // Write chunk to stream
  writeStream.write(chunkText + ' ');

  // Progress logging every 5000 entries
  if (transcriptEntries.length > PROGRESS_THRESHOLD && (i + CHUNK_SIZE) % 5000 === 0) {
    const processed = Math.min(i + CHUNK_SIZE, transcriptEntries.length);
    console.error(`Progress: ${processed}/${transcriptEntries.length} entries`);
  }
}

// Close stream and wait for completion
await new Promise<void>((resolve, reject) => {
  writeStream.end(() => {
    console.error(`Transcript saved to: ${absoluteOutputPath}`);
    resolve();
  });
  writeStream.on('error', reject);
});

// Memory monitoring (gated by DEBUG env var)
if (process.env.DEBUG?.includes('memory') && memoryBefore) {
  const memoryAfter = process.memoryUsage();
  console.error(`Memory after streaming: ${Math.round(memoryAfter.heapUsed / 1024 / 1024)}MB`);
  console.error(`Peak memory delta: ${Math.round((memoryAfter.heapUsed - memoryBefore.heapUsed) / 1024 / 1024)}MB`);
}
```

**Pattern Adherence:**
- Uses `console.error()` for all diagnostic output (preserves MCP protocol contract)
- Maintains existing error handling pattern with `McpError` and `ErrorCode.InternalError`
- Preserves path resolution logic (absolute paths, relative return values)
- Keeps directory creation pattern (`fs.mkdir` with `recursive: true`)
- Maintains string sanitization approach (chained `.replace()` calls)
- Follows event-based error handling pattern (adapted from postgres-mcp-server pg_agent.ts:46)
- Uses batch processing pattern (adapted from knowledge-manager crawler.py:245-274)

---

## New Files to Create

### File 1: `servers/binaries/youtube-mcp-server/tests/streaming.test.ts`

**Purpose:** Comprehensive test suite for streaming implementation

**Contents:**
```typescript
import { describe, it, expect, beforeEach, afterEach } from 'vitest';
import fs from 'fs/promises';
import path from 'path';
import { createWriteStream } from 'fs';
import he from 'he';

// Mock transcript entry type
interface TranscriptEntry {
  text: string;
  duration: number;
  offset: number;
}

describe('YouTube Transcript Streaming', () => {
  const TEST_OUTPUT_DIR = path.join(__dirname, '../test-output');

  beforeEach(async () => {
    await fs.mkdir(TEST_OUTPUT_DIR, { recursive: true });
  });

  afterEach(async () => {
    await fs.rm(TEST_OUTPUT_DIR, { recursive: true, force: true });
  });

  describe('Chunk Processing', () => {
    it('should process 1000 entries in single chunk', () => {
      const entries: TranscriptEntry[] = Array.from({ length: 1000 }, (_, i) => ({
        text: `word${i}`,
        duration: 1,
        offset: i
      }));

      const CHUNK_SIZE = 1000;
      const chunks = [];
      for (let i = 0; i < entries.length; i += CHUNK_SIZE) {
        chunks.push(entries.slice(i, i + CHUNK_SIZE));
      }

      expect(chunks.length).toBe(1);
      expect(chunks[0].length).toBe(1000);
    });

    it('should process 10000 entries in 10 chunks', () => {
      const entries: TranscriptEntry[] = Array.from({ length: 10000 }, (_, i) => ({
        text: `word${i}`,
        duration: 1,
        offset: i
      }));

      const CHUNK_SIZE = 1000;
      const chunks = [];
      for (let i = 0; i < entries.length; i += CHUNK_SIZE) {
        chunks.push(entries.slice(i, i + CHUNK_SIZE));
      }

      expect(chunks.length).toBe(10);
      expect(chunks[9].length).toBe(1000);
    });

    it('should handle partial final chunk (60001 entries = 61 chunks)', () => {
      const entries: TranscriptEntry[] = Array.from({ length: 60001 }, (_, i) => ({
        text: `word${i}`,
        duration: 1,
        offset: i
      }));

      const CHUNK_SIZE = 1000;
      const chunks = [];
      for (let i = 0; i < entries.length; i += CHUNK_SIZE) {
        chunks.push(entries.slice(i, i + CHUNK_SIZE));
      }

      expect(chunks.length).toBe(61);
      expect(chunks[60].length).toBe(1); // last chunk has 1 entry
    });
  });

  describe('HTML Entity Decoding', () => {
    it('should decode numeric apostrophe entity', () => {
      const text = "It&#39;s working";
      const decoded = text.replace(/&#39;/g, "'");
      expect(decoded).toBe("It's working");
    });

    it('should decode named apostrophe entity', () => {
      const text = "It's working";
      const decoded = text.replace(/'/g, "'");
      expect(decoded).toBe("It's working");
    });

    it('should decode all HTML entities via he.decode', () => {
      const text = "Test &lt;tag&gt; &amp; &quot;quotes&quot;";
      const decoded = he.decode(text);
      expect(decoded).toBe('Test <tag> & "quotes"');
    });
  });

  describe('Memory Usage', () => {
    it('should maintain <100MB peak for 60k entries', async () => {
      const entries: TranscriptEntry[] = Array.from({ length: 60000 }, (_, i) => ({
        text: `word${i} test content with some length to simulate real transcript`,
        duration: 1,
        offset: i
      }));

      const outputPath = path.join(TEST_OUTPUT_DIR, 'memory-test.md');
      const CHUNK_SIZE = 1000;

      const memBefore = process.memoryUsage();

      const writeStream = createWriteStream(outputPath, { encoding: 'utf-8' });
      writeStream.write('# Test Transcript\n\n');

      for (let i = 0; i < entries.length; i += CHUNK_SIZE) {
        const chunk = entries.slice(i, i + CHUNK_SIZE);
        const chunkText = chunk
          .map(entry => he.decode(entry.text.replace(/&#39;/g, "'")))
          .join(' ');
        writeStream.write(chunkText + ' ');
      }

      await new Promise<void>((resolve, reject) => {
        writeStream.end(() => resolve());
        writeStream.on('error', reject);
      });

      const memAfter = process.memoryUsage();
      const peakDelta = (memAfter.heapUsed - memBefore.heapUsed) / 1024 / 1024;

      expect(peakDelta).toBeLessThan(100);
    });
  });

  describe('Progress Logging', () => {
    it('should trigger progress logs for >5000 entries', () => {
      const entries: TranscriptEntry[] = Array.from({ length: 10000 }, (_, i) => ({
        text: `word${i}`,
        duration: 1,
        offset: i
      }));

      const CHUNK_SIZE = 1000;
      const PROGRESS_THRESHOLD = 5000;
      const progressLogs: string[] = [];

      for (let i = 0; i < entries.length; i += CHUNK_SIZE) {
        if (entries.length > PROGRESS_THRESHOLD && (i + CHUNK_SIZE) % 5000 === 0) {
          const processed = Math.min(i + CHUNK_SIZE, entries.length);
          progressLogs.push(`Progress: ${processed}/${entries.length} entries`);
        }
      }

      expect(progressLogs.length).toBeGreaterThan(0);
      expect(progressLogs).toContain('Progress: 5000/10000 entries');
      expect(progressLogs).toContain('Progress: 10000/10000 entries');
    });

    it('should NOT trigger progress logs for ≤5000 entries', () => {
      const entries: TranscriptEntry[] = Array.from({ length: 5000 }, (_, i) => ({
        text: `word${i}`,
        duration: 1,
        offset: i
      }));

      const CHUNK_SIZE = 1000;
      const PROGRESS_THRESHOLD = 5000;
      const progressLogs: string[] = [];

      for (let i = 0; i < entries.length; i += CHUNK_SIZE) {
        if (entries.length > PROGRESS_THRESHOLD && (i + CHUNK_SIZE) % 5000 === 0) {
          const processed = Math.min(i + CHUNK_SIZE, entries.length);
          progressLogs.push(`Progress: ${processed}/${entries.length} entries`);
        }
      }

      expect(progressLogs.length).toBe(0);
    });
  });

  describe('Filename Generation', () => {
    it('should sanitize special characters', () => {
      const text = "Test's Video! (2025) @Channel";
      const sanitized = text
        .trim()
        .toLowerCase()
        .replace(/\s+/g, '-')
        .replace(/[^a-z0-9-]/g, '');

      expect(sanitized).toBe('tests-video-2025-channel');
    });

    it('should fallback to timestamp for empty/invalid input', () => {
      const text = "!@#$%^&*()";
      let baseFilename = text
        .trim()
        .toLowerCase()
        .replace(/\s+/g, '-')
        .replace(/[^a-z0-9-]/g, '');

      if (!baseFilename || baseFilename === '-') {
        baseFilename = `transcript-${Date.now()}`;
      }

      expect(baseFilename).toMatch(/^transcript-\d+$/);
    });
  });

  describe('Stream Error Handling', () => {
    it('should handle write stream errors gracefully', async () => {
      const outputPath = '/invalid/path/that/does/not/exist/test.md';
      const writeStream = createWriteStream(outputPath);

      const errorPromise = new Promise((resolve, reject) => {
        writeStream.on('error', (err) => resolve(err));
        writeStream.write('test content');
      });

      const error = await errorPromise;
      expect(error).toBeInstanceOf(Error);
    });
  });
});
```

---

### File 2: `servers/binaries/youtube-mcp-server/vitest.config.ts`

**Purpose:** Vitest configuration for test suite

**Contents:**
```typescript
import { defineConfig } from 'vitest/config';

export default defineConfig({
  test: {
    globals: true,
    environment: 'node',
    coverage: {
      provider: 'v8',
      reporter: ['text', 'html'],
      exclude: ['build/**', 'tests/**', 'vitest.config.ts'],
    },
  },
});
```

---

### File 3: `servers/binaries/youtube-mcp-server/package.json` (modification)

**Purpose:** Add Vitest dev dependencies and test scripts

**Changes:**
```json
{
  "scripts": {
    "test": "vitest run",
    "test:watch": "vitest",
    "test:coverage": "vitest run --coverage"
  },
  "devDependencies": {
    "vitest": "^1.0.0",
    "@vitest/ui": "^1.0.0",
    "@types/node": "^20.0.0"
  }
}
```

---

## Test Strategy

### AC Coverage Map

| AC | Test Type | Test Location | Test Method | Existing/New |
|----|-----------|---------------|-------------|--------------|
| AC1 | Memory | `tests/streaming.test.ts` | Process 50k entry mock array, assert `process.memoryUsage()` delta <100MB | New |
| AC2 | Memory | `tests/streaming.test.ts` | Process 60k entry mock array, assert `process.memoryUsage()` delta <100MB | New |
| AC3 | Unit | `tests/streaming.test.ts` | Verify progress logs appear for 10k entries, not for 5k entries | New |
| AC4 | Unit | `tests/streaming.test.ts` | Verify `createWriteStream()` used, file written in chunks | New |
| AC5 | Integration | Manual | Request 30min real YouTube video, verify completion time similar to current implementation | New |
| AC6 | Unit | `tests/streaming.test.ts` | Mock stream write error, verify error message distinguishes error types | New |

---

### Test Infrastructure

**Framework:** Vitest (lightweight, TypeScript-native, fast)
**No Mocks:** Use real file system for stream writes, real `he.decode()` for HTML entities
**Memory Testing:** Use `process.memoryUsage()` to measure heap usage before/after streaming
**Integration Testing:** Manual testing with real YouTube videos (URLs in test plan comments)

**Test Execution:**
```bash
# Install dependencies
npm install

# Run unit tests
npm test

# Run with coverage
npm run test:coverage

# Watch mode (development)
npm run test:watch
```

---

## Regression Test Strategy

**Purpose:** Verify NO existing functionality breaks when implementing streaming transcript processing

**Coverage Target:** 100% of affected features from research phase Impact Analysis

---

### Affected Feature 1: Transcript Fetching and Saving

**Blast Radius:** `index.ts:114-173` - Main transcript processing pipeline
**Regression Risk:** Transcript files could be incomplete, corrupted, or fail to write

**Unit Tests (New):**
- [ ] **Test:** `should process 1000 entries in single chunk`
  - **File:** `tests/streaming.test.ts`
  - **Test Name:** `describe('Chunk Processing') > should process 1000 entries in single chunk`
  - **Input:** Mock array of 1000 transcript entries
  - **Expected:** Single chunk created with all 1000 entries
  - **Why:** Verifies chunking logic works for normal-length videos

- [ ] **Test:** `should process 10000 entries in 10 chunks`
  - **File:** `tests/streaming.test.ts`
  - **Test Name:** `describe('Chunk Processing') > should process 10000 entries in 10 chunks`
  - **Input:** Mock array of 10000 transcript entries
  - **Expected:** 10 chunks created, each with 1000 entries
  - **Why:** Verifies chunking logic scales for longer videos

- [ ] **Test:** `should handle partial final chunk (60001 entries)`
  - **File:** `tests/streaming.test.ts`
  - **Test Name:** `describe('Chunk Processing') > should handle partial final chunk`
  - **Input:** Mock array of 60001 transcript entries
  - **Expected:** 61 chunks created, last chunk has 1 entry
  - **Why:** Verifies edge case where final chunk is incomplete

**Integration Tests (New):**
- [ ] **Test:** Real YouTube video transcript processing
  - **File:** Manual testing (document in test notes)
  - **Scenario:** Request transcript for 30min video via MCP client
  - **Expected:** File written successfully, content matches video
  - **NO MOCKS:** Use actual YouTube API via youtube-transcript library

**Manual Verification:**
- [ ] Download 30min video transcript, verify file size ~500KB, content readable
- [ ] Download 5hr video transcript, verify file size ~5MB, content readable
- [ ] Download 6hr video transcript, verify file size ~6MB, content readable

---

### Affected Feature 2: Error Handling for YouTube API Failures

**Blast Radius:** `index.ts:185-211` - YouTube-specific error message matching
**Regression Risk:** Error messages could change or disappear

**Unit Tests (New):**
- [ ] **Test:** `should handle write stream errors gracefully`
  - **File:** `tests/streaming.test.ts`
  - **Test Name:** `describe('Stream Error Handling') > should handle write stream errors`
  - **Input:** Invalid file path `/invalid/path/that/does/not/exist/test.md`
  - **Expected:** Error event fired with Error instance
  - **Why:** Verifies stream error handling preserves error reporting

- [ ] **Test:** Error message parsing still works for TranscriptsDisabled
  - **File:** Integration test (manual verification)
  - **Input:** YouTube video with disabled transcripts
  - **Expected:** Error message contains "Transcripts are disabled"
  - **Why:** Verifies error message matching logic unchanged

**Integration Tests (New):**
- [ ] **Test:** YouTube API errors return `isError: true`
  - **File:** Manual testing
  - **Scenario:** Request transcript for private/deleted YouTube video
  - **Expected:** MCP response has `isError: true`, error message describes issue
  - **NO MOCKS:** Use actual youtube-transcript library error responses

**Manual Verification:**
- [ ] Test with disabled/private YouTube video, verify error message format unchanged

---

### Affected Feature 3: File Path Resolution and Directory Creation

**Blast Radius:** `index.ts:164-171` - Path resolution and directory creation
**Regression Risk:** Files saved to wrong location, missing directories

**Unit Tests (New):**
- [ ] **Test:** Path resolution logic unchanged
  - **File:** `tests/streaming.test.ts` (add new describe block)
  - **Test Name:** `describe('Path Resolution') > should resolve absolute paths correctly`
  - **Input:** Relative path `transcripts/test.md`
  - **Expected:** Absolute path using `path.resolve(CLINE_CWD, ...)`
  - **Why:** Verifies path resolution logic preserved

- [ ] **Test:** Directory creation works for nested paths
  - **File:** `tests/streaming.test.ts`
  - **Test Name:** `describe('Path Resolution') > should create nested directories`
  - **Input:** Path `a/b/c/transcript.md`
  - **Expected:** All directories created with `fs.mkdir(recursive: true)`
  - **Why:** Verifies directory creation logic preserved

**Integration Tests (New):**
- [ ] **Test:** Nested output paths work via MCP client
  - **File:** Manual testing
  - **Scenario:** Request transcript with `output_path: "transcripts/2025/november/test.md"`
  - **Expected:** All directories created, file saved to correct location
  - **NO MOCKS:** Use real file system operations

**Manual Verification:**
- [ ] Check transcript saved to expected location with nested path

---

### Affected Feature 4: Filename Generation from Transcript Content

**Blast Radius:** `index.ts:140-162` - Title and filename generation
**Regression Risk:** Filenames could be truncated or use fallback more often

**Unit Tests (New):**
- [ ] **Test:** `should sanitize special characters`
  - **File:** `tests/streaming.test.ts`
  - **Test Name:** `describe('Filename Generation') > should sanitize special characters`
  - **Input:** Text "Test's Video! (2025) @Channel"
  - **Expected:** Sanitized to "tests-video-2025-channel"
  - **Why:** Verifies filename sanitization logic preserved

- [ ] **Test:** `should fallback to timestamp for empty/invalid input`
  - **File:** `tests/streaming.test.ts`
  - **Test Name:** `describe('Filename Generation') > should fallback to timestamp`
  - **Input:** Text "!@#$%^&*()"
  - **Expected:** Filename matches pattern `transcript-\d+`
  - **Why:** Verifies fallback logic for invalid inputs

**Integration Tests (New):**
- [ ] **Test:** Filenames with special characters in first entry
  - **File:** Manual testing
  - **Scenario:** Request transcript for video with emoji/special chars in first entry
  - **Expected:** Filename sanitized correctly, no illegal characters
  - **NO MOCKS:** Use real youtube-transcript data

**Manual Verification:**
- [ ] Inspect generated filenames for 5 real videos with various first-entry content

---

### Affected Feature 5: MCP Protocol Response Structure

**Blast Radius:** `index.ts:175-184` - Success response with relative path
**Regression Risk:** Response structure change breaks MCP clients

**Unit Tests (New):**
- [ ] **Test:** Return value structure unchanged
  - **File:** `tests/streaming.test.ts` (add new describe block)
  - **Test Name:** `describe('MCP Response') > should return correct structure`
  - **Input:** Mock successful transcript write
  - **Expected:** `{ content: [{ type: 'text', text: '...' }] }` structure
  - **Why:** Verifies MCP response schema preserved

**Integration Tests (New):**
- [ ] **Test:** MCP inspector validates response JSON
  - **File:** Manual testing with MCP inspector tool
  - **Scenario:** Request transcript via MCP client
  - **Expected:** Response passes MCP schema validation
  - **NO MOCKS:** Use actual MCP client/server communication

---

### Affected Feature 6: Progress Logging to stderr

**Blast Radius:** New progress logging during streaming
**Regression Risk:** Excessive logging could spam stderr

**Unit Tests (New):**
- [ ] **Test:** `should trigger progress logs for >5000 entries`
  - **File:** `tests/streaming.test.ts`
  - **Test Name:** `describe('Progress Logging') > should trigger progress logs`
  - **Input:** Mock array of 10000 entries
  - **Expected:** Progress logs at 5000 and 10000 entries
  - **Why:** Verifies progress logging threshold works

- [ ] **Test:** `should NOT trigger progress logs for ≤5000 entries`
  - **File:** `tests/streaming.test.ts`
  - **Test Name:** `describe('Progress Logging') > should NOT trigger progress logs`
  - **Input:** Mock array of 5000 entries
  - **Expected:** Zero progress logs
  - **Why:** Verifies progress logging doesn't spam for short transcripts

**Integration Tests (New):**
- [ ] **Test:** Capture stderr during 10k entry test
  - **File:** Manual testing
  - **Scenario:** Request transcript for long video, capture stderr output
  - **Expected:** Progress logs appear every 5000 entries
  - **NO MOCKS:** Use real MCP server execution

**Manual Verification:**
- [ ] Watch stderr output for 6hr video, verify log frequency acceptable (~12 logs)

---

### Regression Test Coverage Summary

**Total Affected Features:** 6
**Total Unit Tests Required:** 11
**Total Integration Tests Required:** 6
**Total Manual Checks Required:** 6
**Coverage Percentage:** 100% (all affected features tested)

**Verification Command:** `/sop-regression-verification 2` (run after implementation)

**Zero Tolerance Policy:**
- ALL 23 tests must PASS before PR creation
- ANY failure = PR BLOCKED until fixed
- Evidence required (test logs, screenshots for manual verification)

---

## Implementation Checklist

**Status:** `[ ]` not started, `[→]` in progress, `[✓]` done, `[!]` blocked

### Setup
- [ ] Create feature branch `feature/002-streaming-transcripts`
- [ ] Install Vitest dev dependencies: `npm install -D vitest @vitest/ui @types/node`
- [ ] Create `vitest.config.ts`
- [ ] Create `tests/streaming.test.ts` with all unit tests
- [ ] Run existing tests (none exist currently) → establish baseline

### Modification 1: Add createWriteStream Import
- [ ] Read `index.ts:1-13` (imports section)
- [ ] Add `import { createWriteStream } from 'fs';` after `import fs from 'fs/promises';`
- [ ] Verify TypeScript compilation: `npm run build`
- [ ] Run tests → PASS (no functional changes yet)

### Modification 2: Replace Transcript Processing Logic
- [ ] Read `index.ts:130-173` (current transcript processing)
- [ ] Replace with streaming implementation per Target Code above
- [ ] Preserve existing patterns:
  - [ ] Use `console.error()` for all diagnostic output
  - [ ] Maintain error handling with `McpError`
  - [ ] Keep path resolution logic unchanged
  - [ ] Preserve directory creation pattern
  - [ ] Maintain string sanitization approach
- [ ] Add configuration constants (`CHUNK_SIZE`, `PROGRESS_THRESHOLD`)
- [ ] Add environment-gated memory logging
- [ ] Add stream error handling with partial file cleanup
- [ ] Verify TypeScript compilation: `npm run build`

### Testing Phase 1: Unit Tests
- [ ] Run unit tests: `npm test`
- [ ] Fix: Chunk processing tests → ALL PASS
- [ ] Fix: HTML entity decoding tests → ALL PASS
- [ ] Fix: Memory usage test (<100MB for 60k entries) → PASS
- [ ] Fix: Progress logging tests → ALL PASS
- [ ] Fix: Filename generation tests → ALL PASS
- [ ] Fix: Stream error handling test → PASS
- [ ] Run full suite → ALL PASS (11/11 unit tests)

### Testing Phase 2: Integration Tests (Manual)
- [ ] Test 1: 30min YouTube video (e.g., `https://youtube.com/watch?v=<30min-video>`)
  - [ ] Request transcript via MCP client
  - [ ] Verify file written successfully
  - [ ] Verify content matches video
  - [ ] Verify completion time similar to current implementation (AC5)
- [ ] Test 2: 5hr YouTube video (e.g., `https://youtube.com/watch?v=<5hr-video>`)
  - [ ] Request transcript via MCP client
  - [ ] Verify file written successfully (~5MB)
  - [ ] Monitor stderr for progress logs
  - [ ] Verify memory usage <100MB (use `DEBUG=memory` flag) (AC1)
- [ ] Test 3: 6hr YouTube video (e.g., `https://youtube.com/watch?v=<6hr-video>`)
  - [ ] Request transcript via MCP client
  - [ ] Verify file written successfully (~6MB)
  - [ ] Monitor stderr for progress logs
  - [ ] Verify memory usage <100MB (use `DEBUG=memory` flag) (AC2)
- [ ] Test 4: Error handling
  - [ ] Test with disabled/private video
  - [ ] Verify error message format preserved (AC6)
  - [ ] Verify `isError: true` in MCP response

### Regression Verification
- [ ] Run comprehensive regression test suite (23 tests total)
- [ ] Execute `/sop-regression-verification 2`
- [ ] Verify ALL 6 affected features pass regression tests
- [ ] Document evidence (test logs, screenshots)
- [ ] Fix ANY failures before proceeding

### Final Verification
- [ ] All spec requirements met (6 ACs PASS)
- [ ] No regressions (23 regression tests PASS)
- [ ] Code follows existing patterns (verified via code review)
- [ ] Update `package.json` with test scripts
- [ ] Run full build: `npm run build` → SUCCESS
- [ ] Run full test suite: `npm test` → ALL PASS
- [ ] Ready for commit and PR

---

## Definition of Done

**Functional Requirements:**
- [ ] AC1: 5-hour video processes successfully, memory <100MB peak
- [ ] AC2: 6-hour video processes successfully, memory <100MB peak
- [ ] AC3: Progress logs appear for transcripts >5000 entries
- [ ] AC4: Uses `createWriteStream()` for file writes (no atomic `fs.writeFile()`)
- [ ] AC5: No performance regression for 30-minute videos
- [ ] AC6: Error messages distinguish timeout from other YouTube API failures

**Testing Requirements:**
- [ ] All 11 unit tests passing
- [ ] All 6 integration tests passing (manual verification documented)
- [ ] All 23 regression tests passing (100% affected feature coverage)
- [ ] Memory tests validate <100MB peak for 60k entries
- [ ] Test coverage >80% (run `npm run test:coverage`)

**Code Quality:**
- [ ] TypeScript compilation successful (`npm run build`)
- [ ] No new linter errors
- [ ] Follows existing code patterns (error logging, path resolution, sanitization)
- [ ] Stream error handling with partial file cleanup implemented
- [ ] Environment-gated memory logging implemented (`DEBUG=memory`)

**Documentation:**
- [ ] Test suite documented in `tests/streaming.test.ts`
- [ ] Integration test URLs documented in test comments
- [ ] `package.json` updated with test scripts
- [ ] Memory monitoring usage documented (README update if needed)

**Regression Testing:**
- [ ] Regression Test Strategy section complete with ALL 6 affected features from research
- [ ] ALL 6 affected features have Unit + Integration + Manual test plans
- [ ] Regression test execution plan references `/sop-regression-verification`
- [ ] 100% blast radius coverage documented
- [ ] Zero tolerance for critical path test failures

---

**Ready for Review:** All DoD criteria met, all tests passing, no regressions detected
